{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEATY Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T04:34:08.727067Z",
     "start_time": "2017-12-06T04:34:08.710057Z"
    },
    "cell_style": "center",
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from MEATY_backend import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as mpatches\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "import queue\n",
    "import threading\n",
    "from pytesseract import image_to_string\n",
    "from PIL import Image\n",
    "from itertools import combinations as combos\n",
    "import imagehash as ihash\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_date(x): return x.date()\n",
    "def get_dayofweek(x): return x.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T04:34:08.752859Z",
     "start_time": "2017-12-06T04:34:08.728353Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def hash_img(post_id, group_folder):\n",
    "    \"\"\" creates a wavelet image hash to find duplicate\n",
    "    or similar posted imgs\"\"\"\n",
    "    img_fn = group_folder + '/imgs/' + post_id + '.jpg'\n",
    "    img = Image.open(img_fn)\n",
    "    return (post_id, str(ihash.whash(img, hash_size=16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T04:34:08.787166Z",
     "start_time": "2017-12-06T04:34:08.754216Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def hash_imgs_in_group(group_name='cornell', save_csv=True):\n",
    "    \"\"\" hashes all of the images from the given school's posts \"\"\"\n",
    "    group_folder = '../' + group_name\n",
    "    backward_scrape_exists = os.path.exists(\n",
    "        group_folder + '/temp_memedata_backward.csv')\n",
    "    forward_scrape_exists = os.path.exists(\n",
    "        group_folder + '/temp_memedata_forward.csv')\n",
    "    if backward_scrape_exists:\n",
    "        dfb = pd.read_csv(group_folder + '/temp_memedata_backward.csv')\n",
    "    if forward_scrape_exists:\n",
    "        dff = pd.read_csv(group_folder + '/temp_memedata_forward.csv')\n",
    "    if forward_scrape_exists and backward_scrape_exists:\n",
    "        print('--> Found both backward and forward files.')\n",
    "        df = pd.concat([dfb, dff])\n",
    "    elif forward_scrape_exists:\n",
    "        print('--> Found only a forward file.')\n",
    "        df = dff\n",
    "    elif backward_scrape_exists:\n",
    "        print('--> Found only a backward file.')\n",
    "        df = dfb\n",
    "    else:\n",
    "        print('ERROR: Cannot find files!')\n",
    "        return\n",
    "    list_ids = df['id'].values\n",
    "    imhash = [hash_img(post_id, group_folder) for post_id in list_ids]\n",
    "    df['post_time'] = pd.DatetimeIndex(df['post_time'])\n",
    "    df['post_date'] = df['post_time'].apply(get_date)\n",
    "    df['post_hour'] = df['post_time'].apply(lambda x: x.hour)\n",
    "    df.sort_values(by='post_time', inplace=True)\n",
    "    hash_df = pd.DataFrame(imhash, columns=['id', 'img_hash'])\n",
    "    df_hashed = pd.merge(df, hash_df)\n",
    "    df_hashed = dedupe_raw_hashed(df_hashed)\n",
    "    if save_csv:\n",
    "        df_hashed.to_csv(\n",
    "            group_folder + '/raw_memedata_hashed.csv', index=False)\n",
    "    return df_hashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T04:34:08.814688Z",
     "start_time": "2017-12-06T04:34:08.788907Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def process_raw_posts(list_of_groups=['cornell', 'harvard', 'yale', 'princeton',\n",
    "                                      'columbia', 'dartmouth', 'penn', 'brown'],\n",
    "                      save_csv=True):\n",
    "    \"\"\" preprocess many groups' post data in ~parallel~ \"\"\"\n",
    "    df_list = threads(8, list_of_groups, hash_imgs_in_group)\n",
    "    df_all = pd.concat(df_list)\n",
    "    df_all.set_index('id', inplace=True)\n",
    "    if save_csv:\n",
    "        df_all.to_csv('../all_memedata_dedupe.csv')\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T04:34:08.842599Z",
     "start_time": "2017-12-06T04:34:08.816263Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def aggregate_member_data(list_of_groups=['cornell', 'harvard', 'yale', 'princeton',\n",
    "                                          'columbia', 'dartmouth', 'penn', 'brown'],\n",
    "                          save_csv=True):\n",
    "    \"\"\" aggregates the output from multiple groups' member data \"\"\"\n",
    "    \n",
    "    def strp_date(x): return datetime.datetime.strptime(x, '%B %d, %Y').date()\n",
    "    df_list = []\n",
    "    for group_name in list_of_groups:\n",
    "        member_data_path = '../' + group_name + '/memberdata.csv'\n",
    "        df = pd.read_csv(member_data_path, parse_dates=['date'])\n",
    "        df['group'] = group_name\n",
    "        df.sort_values('date', inplace=True)\n",
    "        df_list.append(df)\n",
    "    df_all = pd.concat(df_list)\n",
    "    df_all['date'].loc[df_all['date'].str.contains('about', na=False)] = '2017-10-30'\n",
    "    df_all['date'].loc[df_all['date'] == 'February 4, 2017'] = '2017-02-04'\n",
    "    df_all['date'] = pd.DatetimeIndex(df_all['date'])\n",
    "    if save_csv:\n",
    "        df_all.to_csv('../all_member_data.csv')\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T04:36:40.618669Z",
     "start_time": "2017-12-06T04:36:40.612121Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def dedupe_raw_hashed(df):\n",
    "    \"\"\" deduplicates posts based on img hash, post time, poster name, and post url \"\"\"\n",
    "    df_dedupe = df.drop_duplicates(\n",
    "        ['img_hash', 'post_time', 'poster_name', 'url'], keep='first')\n",
    "    return df_dedupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After scraping all posts for some group(s), run:\n",
    "  - **process_raw_posts**( list_of_groups, save_csv=True )\n",
    "  \n",
    "### After scraping all member data for some group(s), run:\n",
    "  - **aggregate_member_data**( list_of_groups, save_csv=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "member_df = aggregate_member_data()\n",
    "posts_df = process_raw_posts()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
