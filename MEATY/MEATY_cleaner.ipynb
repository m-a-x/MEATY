{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEATY Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-02T01:49:40.694691Z",
     "start_time": "2017-12-02T01:49:40.677682Z"
    },
    "cell_style": "center",
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from MEATY_backend import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as mpatches\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "import queue\n",
    "import threading\n",
    "from pytesseract import image_to_string\n",
    "from PIL import Image\n",
    "from itertools import combinations as combos\n",
    "import imagehash as ihash\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_date(x): return x.date()\n",
    "def get_dayofweek(x): return x.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-02T01:49:51.534586Z",
     "start_time": "2017-12-02T01:49:51.530797Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def hash_img(post_id=None, group_folder=None):\n",
    "    def make_fn(x): return group_folder + '/imgs/' + x + '.jpg'\n",
    "    img_fn = make_fn(post_id)\n",
    "    img = Image.open(img_fn)\n",
    "    return (post_id, str(ihash.whash(img, hash_size=32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-02T01:49:40.763255Z",
     "start_time": "2017-12-02T01:49:40.724628Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def hash_raw_data(group_name='cornell', save_csv=True):\n",
    "    group_folder = '../' + group_name\n",
    "    backward_scrape_exists = os.path.exists(\n",
    "        group_folder + '/temp_memedata_backward.csv')\n",
    "    forward_scrape_exists = os.path.exists(\n",
    "        group_folder + '/temp_memedata_forward.csv')\n",
    "    if backward_scrape_exists:\n",
    "        dfb = pd.read_csv(group_folder + '/temp_memedata_backward.csv')\n",
    "    if forward_scrape_exists:\n",
    "        dff = pd.read_csv(group_folder + '/temp_memedata_forward.csv')\n",
    "    if forward_scrape_exists and backward_scrape_exists:\n",
    "        print('--> Found both backward and forward files.')\n",
    "        df = pd.concat([dfb, dff])\n",
    "    elif forward_scrape_exists:\n",
    "        print('--> Found only a forward file.')\n",
    "        df = dff\n",
    "    elif backward_scrape_exists:\n",
    "        print('--> Found only a backward file.')\n",
    "        df = dfb\n",
    "    else:\n",
    "        print('ERROR: Cannot find files!')\n",
    "        return\n",
    "    list_ids = df['id'].values\n",
    "    imhash = [(post_id, str(ihash.whash(Image.open(group_folder + '/imgs/' + post_id + '.jpg'),\n",
    "                                        hash_size=32)))\n",
    "              for post_id in list_ids]\n",
    "    df['post_time'] = pd.DatetimeIndex(df['post_time'])\n",
    "    df['post_date'] = df['post_time'].apply(get_date)\n",
    "    df['post_hour'] = df['post_time'].apply(lambda x: x.hour)\n",
    "    df = dedup_raw_hashed(df)\n",
    "    df.sort_values(by='post_time', inplace=True)\n",
    "    hash_df = pd.DataFrame(imhash, columns=['id', 'img_hash'])\n",
    "    df_hashed = pd.merge(df, hash_df)\n",
    "    if save_csv:\n",
    "        df_hashed.to_csv(\n",
    "            group_folder + '/raw_memedata_hashed.csv', index=False)\n",
    "    return df_hashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-02T01:49:40.819071Z",
     "start_time": "2017-12-02T01:49:40.764720Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def load_dedupe_memedata(group_name=None):\n",
    "    if group_name is None:\n",
    "        try:\n",
    "            raw_data_path = 'all_memedata_dedupe.csv'\n",
    "        except:\n",
    "            print('ERROR: could not load file')\n",
    "    else:\n",
    "        raw_data_path = group_name + '/raw_memedata_hashed.csv'\n",
    "    raw_data_path = \"../\" + raw_data_path\n",
    "    df = pd.read_csv(raw_data_path)\n",
    "    df['post_time'] = pd.DatetimeIndex(df['post_time'])\n",
    "    df = dedupe_raw_hashed(df)\n",
    "    if group_name:\n",
    "        df['group'] = group_name\n",
    "    else:\n",
    "        df['group'] = df.group.str.lstrip('../')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-02T01:49:40.843810Z",
     "start_time": "2017-12-02T01:49:40.821273Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def dedupe_combine_groups(list_of_groups=['cornell', 'harvard', 'yale', 'princeton',\n",
    "                                          'columbia', 'dartmouth', 'penn', 'brown'],\n",
    "                          save_csv=True):\n",
    "\n",
    "    df_list = threads(8, list_of_groups, hash_raw_data)\n",
    "    df_all = pd.concat(df_list)\n",
    "    df_all.set_index('id', inplace=True)\n",
    "    if save_csv:\n",
    "        df_all.to_csv('../all_memedata_dedupe.csv')\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-02T01:50:23.475663Z",
     "start_time": "2017-12-02T01:50:23.445408Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def aggregate_member_data(list_of_groups=['cornell', 'harvard', 'yale', 'princeton',\n",
    "                                          'columbia', 'dartmouth', 'penn', 'brown'],\n",
    "                          save_csv=True):\n",
    "    def strp_date(x): return datetime.datetime.strptime(x, '%B %d, %Y').date()\n",
    "    df_list = []\n",
    "    for group_name in list_of_groups:\n",
    "        member_data_path = '../' + group_name + '/memberdata.csv'\n",
    "        df = pd.read_csv(member_data_path, parse_dates=['date'])\n",
    "        df['group'] = group_name\n",
    "        df.sort_values('date', inplace=True)\n",
    "        df_list.append(df)\n",
    "    df_all = pd.concat(df_list)\n",
    "    df_all['date'].loc[df_all['date'].str.contains(\n",
    "        'about', na=False)] = '2017-10-30'\n",
    "    df_all['date'].loc[df_all['date'] == 'February 4, 2017'] = '2017-02-04'\n",
    "    df_all['date'] = pd.DatetimeIndex(df_all['date'])\n",
    "    if save_csv:\n",
    "        print('saved')\n",
    "        df_all.to_csv('../all_member_data.csv')\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-02T01:50:24.368823Z",
     "start_time": "2017-12-02T01:50:24.361947Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def dedupe_raw_hashed(df):\n",
    "    df_dedupe = df.drop_duplicates(\n",
    "        ['img_hash', 'post_time', 'poster_name', 'url'], keep='first')\n",
    "    return df_dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-02T01:49:40.928324Z",
     "start_time": "2017-12-02T01:49:40.900712Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def reposted_imgs(df):\n",
    "    rep_im_df = df[df.img_hash.isin\n",
    "                   (df.img_hash.value_counts(ascending=False)[df.img_hash.value_counts(ascending=False).values >= 4].index\n",
    "                    )]\n",
    "    return rep_im_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-02T01:49:40.953129Z",
     "start_time": "2017-12-02T01:49:40.929845Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def group_size_over_time(df_all=None, list_of_groups=['cornell', 'harvard', 'yale', 'princeton',\n",
    "                                                      'columbia', 'dartmouth', 'penn', 'brown']):\n",
    "    if df_all is None:\n",
    "        df_all = aggregate_member_data()\n",
    "    count_idx = pd.date_range(df_all['date'].min(), df_all['date'].max())\n",
    "    dic_size = {}\n",
    "    for g in list_of_groups:\n",
    "        gdf = df_all.groupby(['date', 'group']).group.count().groupby('group').cumsum()[:, g].reindex(count_idx,\n",
    "                                                                                                      method='ffill')\n",
    "        dic_size[g] = gdf\n",
    "    size_df = pd.DataFrame(dic_size)\n",
    "#     size_df[size_df.isna()] = 0\n",
    "    return size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-26T02:42:02.152443Z",
     "start_time": "2017-11-26T02:42:02.139089Z"
    }
   },
   "outputs": [],
   "source": [
    "def stackplot_size(count_df, list_of_groups=['harvard', 'columbia', 'yale', 'cornell', 'penn',\n",
    "                                             'princeton', 'dartmouth', 'brown']):\n",
    "    stack_list = []\n",
    "    patch_list = []\n",
    "    color_ord = []\n",
    "    for school in list_of_groups:\n",
    "        print(count_df[school].index.min())\n",
    "        stack_list.append(count_df[school].values)\n",
    "        patch_list.append(mpatches.Patch(color=color_dic[school]))\n",
    "        color_ord.append(color_dic[school])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.stackplot(count_df.index.to_pydatetime(), stack_list, colors=color_ord)\n",
    "    ax.legend(patch_list, list_of_groups, loc='upper left')\n",
    "    ax.set_xlim(left=dt.date(2017, 11, 15),\n",
    "                right=dt.date(2016, 11, 15))\n",
    "    fig.set_size_inches((20, 12))\n",
    "    fig.set_dpi(200)\n",
    "    fig.set_facecolor('lightgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-26T02:42:06.720741Z",
     "start_time": "2017-11-26T02:42:02.290802Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/pandas/core/indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "#       Requires CSV files locally -> should adapt to postgres \n",
    "meme_df = load_dedupe_memedata()\n",
    "df = aggregate_member_data()\n",
    "\n",
    "size_df = group_size_over_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list = threads(8, ['cornell','harvard', 'yale', 'princeton', 'columbia', 'dartmouth', 'penn', 'brown'],\n",
    "#                   hash_raw_data)\n",
    "# for group_name in \n",
    "#     hash_raw_data(group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupim = reposted_imgs(df_al)\n",
    "di_counts = dupim.img_hash.value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupim.url.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ih, n in zip(di_counts.index, di_counts.values):\n",
    "    print(ih[:16], '  ---  ' + str(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfl_d = \n",
    "dfl_d = [dedup_raw_hashed(d) for d in df_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = pd.read_csv('raw_postdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old['df'] = 'old'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old.drop(['img_text', 'post_dayofweek'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old['post_time'] = pd.DatetimeIndex(df_old['post_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = pd.read_csv('../columbia/raw_memedata_hashed-32.csv')\n",
    "dfl['post_time'] = pd.DatetimeIndex(dfl['post_time'])\n",
    "dfl['df'] = 'nu'\n",
    "dfl.drop(['reacts_url', 'thankfuls', 'prides'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([dfl, df_old])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdup = df[df[['img_hash', 'post_time', 'poster_name']].duplicated(keep=False)].sort_values(by='post_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = df.drop_duplicates(['img_hash', 'post_time', 'poster_name', 'url'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odfd = dfd[dfd['df'] == 'old']\n",
    "# odfd[odfd[['num_reacts', 'post_time', 'poster_name']].duplicated()].img_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odfd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odfd.post_date.value_counts(dropna=False).groupby(\"post_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# post_df[post_df[['post_time', 'poster_name', 'title']].duplicated(keep=False)]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
